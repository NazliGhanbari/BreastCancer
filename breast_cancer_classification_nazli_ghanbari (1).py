# -*- coding: utf-8 -*-
"""Breast Cancer Classification-Nazli Ghanbari

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lmGUX6p_XKlTCrUpLoB52Bnaj4oLg2rl

# Breast Cancer Classification

Breast cancer is cancer that develops from breast tissue. Signs of breast cancer may include a lump in the breast, a change in breast shape, dimpling of the skin, fluid coming from the nipple, a newly inverted nipple, or a red or scaly patch of skin. In those with distant spread of the disease, there may be bone pain, swollen lymph nodes, shortness of breath, or yellow skin.

<a id = "2"></a>
# Libraries and Utilities
"""

import numpy as np
import pandas as pd
import seaborn as sns
sns.set_style("darkgrid")
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.neighbors import KNeighborsClassifier, NeighborhoodComponentsAnalysis, LocalOutlierFactor
from sklearn.decomposition import PCA
import os

"""<a id = "3"></a>
# Load and Check Data
"""

data = pd.read_csv('/content/data.csv')
data.head()

"""As It is Shown there are two unsful column to be dropped"""

data.drop(["Unnamed: 32","id"], axis = 1, inplace =True)
data = data.rename(columns = {"diagnosis":"target"})
data.head()

column_names = data.columns.tolist()

print(column_names)

data.describe()

sns.countplot(data["target"])
print(data.target.value_counts())

"""<a id = "4"></a>
## Malignant to 1 and Bening to 0
"""

data["target"] = [1 if str(i).strip() == "M" else 0 for i in data.target]
print("Data Shape:", data.shape)

"""<a id = "5"></a>
# Exploratory Data Analysis

<a id = "6"></a>
## Correlation Map
"""

corr_matrix = data.corr()
f,ax=plt.subplots(figsize = (15,15))
sns.heatmap(corr_matrix,annot= True,fmt = ".1f",ax=ax,cmap='coolwarm',vmin=-1,vmax=1)
plt.xticks(rotation=90)
plt.yticks(rotation=0)
plt.title('Correlation Map', size = 14)
plt.show()

#Visualize the distribution of density of features for both class of points (Two types of cancer)

features_for_distribution = ['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean']

sns.set(style="whitegrid")

fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12, 10))

axes = axes.flatten()

for i, feature in enumerate(features_for_distribution):
    sns.histplot(data[data['target'] == 1][feature], color='blue', label='Malignant', kde=True, ax=axes[i])

    sns.histplot(data[data['target'] == 0][feature], color='orange', label='Benign', kde=True, ax=axes[i])

    axes[i].set_xlabel(feature)
    axes[i].set_ylabel('Density')
    axes[i].set_title(f'Distribution of {feature} by Target Variable')

    axes[i].legend()

plt.tight_layout()
plt.suptitle('Distribution Plots of Selected Features by Target Variable', y=1.02, size=16)
plt.show()

"""<a id = "7"></a>
# Outlier Detection
"""

# drop the target column for future calculation as it is not needed and its going to be fitted on our data and its corresponding points
y = data.target
x = data.drop(["target"], axis = 1)
columns = x.columns.tolist()
x.head()

# using negative outlier factor for identifying outliers in a way of removing negative valued points using a threshhold value
clf = LocalOutlierFactor()
y_pred = clf.fit_predict(x)
X_score = clf.negative_outlier_factor_
outlier_score = pd.DataFrame()
outlier_score["score"] = X_score
outlier_score.head()

# Create scatter plot for 'radius_mean' against 'texture_mean'

sns.set(style="whitegrid")

plt.figure(figsize=(10, 6))
sns.scatterplot(x='radius_mean', y='texture_mean', hue='target', data=data, palette='husl', marker='o', s=70)
plt.title('Scatter Plot: Radius Mean vs. Texture Mean')
plt.xlabel('Radius Mean')
plt.ylabel('Texture Mean')
plt.legend(title='Target', loc='upper right')
plt.show()

# filtering points according to their negative outlier factor score using a threshold score of -2.5
radius = (X_score.max() - X_score) / (X_score.max() - X_score.min())
outlier_score["radius"] = radius
threshold = -1.5
filter_ = outlier_score["score"] < threshold
outlier_index = outlier_score[filter_].index.tolist()
plt.figure(figsize = (12,8))
plt.scatter(x.iloc[outlier_index,0],x.iloc[outlier_index,1], color = "blue", s = 50, label = "Outliers" )
plt.scatter(x.iloc[:,0],x.iloc[:,1], color = "k", s = 3, label = "Data Points" )
plt.scatter(x.iloc[:,0],x.iloc[:,1], s = 1000*radius, edgecolor = "r", facecolors = "none", label ="Outlier Scores")
plt.legend()
plt.show()

"""<a id = "8"></a>
# Drop Outliers
"""

x = x.drop(outlier_index)
y = y.drop(outlier_index).values
x.head()

x.count()

sns.set(style="whitegrid")

plt.figure(figsize=(10, 6))
sns.scatterplot(x='radius_mean', y='texture_mean', hue=y, data=x, palette='husl', marker='o', s=70)
plt.title('Scatter Plot: Radius Mean vs. Texture Mean')
plt.xlabel('Radius Mean')
plt.ylabel('Texture Mean')
plt.legend(title='Target', loc='upper right')
plt.show()

x.describe()

"""<a id = "9"></a>
# Train Test Split
"""

test_size = 0.3
X_train, X_test, Y_train, Y_test = train_test_split(x,y, test_size = test_size, random_state = 42)

import seaborn as sns
import matplotlib.pyplot as plt

# Assuming X_train and X_test are your training and test datasets
# You can replace them with your actual data

# List of features to compare
features_to_compare = ['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean']

# Create subplots in a 2x2 grid
fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12, 10))

# Flatten the axes for easier indexing
axes = axes.flatten()

# Loop through features
for i, feature in enumerate(features_to_compare):
    # Plot overlapping histograms
    sns.histplot(X_train[feature], color='blue', label='Train', kde=True, ax=axes[i])
    sns.histplot(X_test[feature], color='orange', label='Test', kde=True, ax=axes[i])

    # Add labels and title
    axes[i].set_xlabel(feature)
    axes[i].set_ylabel('Density')
    axes[i].set_title(f'Distribution of {feature} in Train and Test Data')

    # Add legend
    axes[i].legend()

# Adjust layout
plt.tight_layout()

# Show the plot
plt.show()
import seaborn as sns
import matplotlib.pyplot as plt

# Assuming X_train and X_test are your training and test datasets
# You can replace them with your actual data

# List of features to compare
features_to_compare = ['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean']

# Create subplots in a 2x2 grid
fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12, 10))

# Flatten the axes for easier indexing
axes = axes.flatten()

# Loop through features
for i, feature in enumerate(features_to_compare):
    # Plot overlapping histograms
    sns.histplot(X_train[feature], color='blue', label='Train', kde=True, ax=axes[i])
    sns.histplot(X_test[feature], color='orange', label='Test', kde=True, ax=axes[i])

    # Add labels and title
    axes[i].set_xlabel(feature)
    axes[i].set_ylabel('Density')
    axes[i].set_title(f'Distribution of {feature} in Train and Test Data')

    # Add legend
    axes[i].legend()

# Adjust layout
plt.tight_layout()

# Show the plot
plt.show()

"""<a id = "10"></a>
# Standardization
"""

# as KNN uses distances if feature for classification and there are various dimensions here, standard the values for this purpose , so there is no dimension
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
x_train_df = pd.DataFrame(X_train, columns = columns)
x_train_df.head()

x_train_df.describe()

"""<a id = "11"></a>
# Basic KNN Method
"""

knn = KNeighborsClassifier(n_neighbors= 2)
knn.fit(X_train,Y_train)
y_pred =  knn.predict(X_test)
cm = confusion_matrix(Y_test, y_pred)
acc = accuracy_score(Y_test, y_pred)
score = knn.score(X_test, Y_test)
print("Score:", score)
print("CM:", cm)
print("Basic KNN Accuracy:", acc)

"""<a id = "12"></a>
## Finding Best KNN Parameters
"""

# DEfine a KNN model with grid function , the utility is that it prevents futre retyping the same syntaxes as it gonna be used several times.
def KNN_Best_Params(x_train, x_test, y_train,y_test):

    k_range = list(range(1,31))
    weight_options = ["uniform","distance"]
    print()
    param_grid = dict(n_neighbors = k_range, weights = weight_options)

    knn = KNeighborsClassifier()
    grid = GridSearchCV(knn, param_grid, cv = 10, scoring = "accuracy")
    grid.fit(x_train, y_train)

    print("Best Training Score: {} with parameters: {}".format(grid.best_score_, grid.best_params_))
    print()

    knn = KNeighborsClassifier(**grid.best_params_)
    knn.fit(x_train,y_train)

    y_pred_test = knn.predict(x_test)
    y_pred_train = knn.predict(x_train)

    cm_test = confusion_matrix(y_test, y_pred_test)
    cm_train = confusion_matrix(y_train, y_pred_train)

    acc_test = accuracy_score(y_test, y_pred_test)
    acc_train = accuracy_score(y_train, y_pred_train)

    print("Test Score: {}, Train Score: {}".format(acc_test,acc_train))

    print()

    print("CM Test:" ,cm_test)
    print("CM Train:", cm_train)

    return grid

grid = KNN_Best_Params(X_train, X_test, Y_train, Y_test)

"""<a id = "13"></a>
# Principal Component Analysis
"""

# using pca model to reduce the dimension of our data as it makes it hard to calculate,also gonna reduce our accurasy !
scaler = StandardScaler()
x_scaled = scaler.fit_transform(x)

pca = PCA(n_components = 2)
pca.fit(x_scaled)
X_reduced_pca = pca.transform(x_scaled)
pca_data = pd.DataFrame(X_reduced_pca, columns = ["p1","p2"])
pca_data["target"] = y

plt.figure(figsize = (14,8))
sns.scatterplot(x = "p1", y = "p2", hue = "target", data = pca_data)
plt.title("PCA: p1 vs p2")
plt.show()

X_train_pca, X_test_pca, Y_train_pca, Y_test_pca = train_test_split(X_reduced_pca, y,test_size = test_size, random_state = 42)
grid_pca = KNN_Best_Params(X_train_pca, X_test_pca, Y_train_pca, Y_test_pca)

# Commented out IPython magic to ensure Python compatibility.
cmap_light = ListedColormap(['orange', 'cornflowerblue'])
cmap_bold = ListedColormap(['darkorange', "darkblue"])

h = 0.05
X = X_reduced_pca
x_min, x_max = X[:,0].min() -1, X[:,0].max() + 1
y_min, y_max = X[:,1].min() -1, X[:,1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                    np.arange(y_min,y_max,h))

Z = grid_pca.predict(np.c_[xx.ravel(), yy.ravel()])

Z = Z.reshape(xx.shape)
plt.figure(figsize = (14,8))
plt.pcolormesh(xx,yy,Z, cmap = cmap_light)

plt.scatter(X[:,0], X[:,1], c = y, cmap = cmap_bold,
           edgecolor = 'k', s = 20)
plt.xlim(xx.min(), xx.max())
plt.ylim(yy.min(), yy.max())
plt.title("%i-Class Classification (k = %i, weights = '%s')"
#          % (len(np.unique(y)), grid_pca.best_estimator_.n_neighbors, grid_pca.best_estimator_.weights))
plt.show()

"""<a id = "14"></a>
# Neighborhood Component Analysis (NCA)
"""

nca = NeighborhoodComponentsAnalysis(n_components = 2, random_state = 42)
nca.fit(x_scaled, y)
X_reduced_nca = nca.transform(x_scaled)
nca_data = pd.DataFrame(X_reduced_nca, columns = ["p1","p2"])
nca_data["target"] = y
plt.figure(figsize = (14,8))
sns.scatterplot(x = "p1", y= "p2", hue = "target", data = nca_data )
plt.title("NCA: p1 vs p2")
plt.show()

X_train_nca, X_test_nca, Y_train_nca, Y_test_nca = train_test_split(X_reduced_nca, y,test_size = test_size, random_state = 42)
grid_nca = KNN_Best_Params(X_train_nca, X_test_nca, Y_train_nca, Y_test_nca)

# Commented out IPython magic to ensure Python compatibility.
cmap_light = ListedColormap(['orange', 'cornflowerblue'])
cmap_bold = ListedColormap(['darkorange', "darkblue"])
h = 0.35
X = X_reduced_nca
x_min, x_max = X[:,0].min() -1, X[:,0].max() + 1
y_min, y_max = X[:,1].min() -1, X[:,1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                    np.arange(y_min,y_max,h))

Z = grid_nca.predict(np.c_[xx.ravel(), yy.ravel()])

Z = Z.reshape(xx.shape)
plt.figure(figsize = (14,8))
plt.pcolormesh(xx,yy,Z, cmap = cmap_light)

plt.scatter(X[:,0], X[:,1], c = y, cmap = cmap_bold,
           edgecolor = 'k', s = 20)
plt.xlim(xx.min(), xx.max())
plt.ylim(yy.min(), yy.max())
plt.title("%i-Class Classification (k = %i, weights = '%s')"
#          % (len(np.unique(y)), grid_nca.best_estimator_.n_neighbors, grid_nca.best_estimator_.weights));